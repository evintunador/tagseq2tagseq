"""
LLM-compiled training loop combining atomic features: grad_accum, loss_tracking
Generated by gpt-lab LLM compiler
Device: mps
"""

# Metadata for discovery and testing
__atomic_features__ = ['grad_accum', 'loss_tracking']
__llm_compiled__ = True

from typing import Dict, Any, List

import torch
import torch.nn as nn


def run_training(
    model: nn.Module,
    optimizer: torch.optim.Optimizer,
    train_loader,
    *,
    # grad accum knobs
    accum_steps: int = 1,
    # loss tracking knobs
    track_loss: bool = False,
    log_interval: int = 1,
    # misc
    **kwargs,
) -> Dict[str, Any]:
    """Training loop combining gradient accumulation and loss tracking."""
    model.train()
    optimizer.zero_grad(set_to_none=True)

    if accum_steps is None or accum_steps < 1:
        accum_steps = 1

    train_loss_history: List[float] = []
    step_count = 0
    micro_idx = 0
    
    for batch in train_loader:
        loss = model(batch)

        if accum_steps > 1:
            loss = loss / float(accum_steps)

        loss.backward()

        micro_idx += 1

        # Track loss if enabled and at the right interval
        if track_loss and (step_count % log_interval == 0):
            # Use the original loss value (before scaling) for tracking
            original_loss = loss * float(accum_steps) if accum_steps > 1 else loss
            train_loss_history.append(float(original_loss.detach().cpu().item()))

        if micro_idx % accum_steps == 0:
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)
            step_count += 1

    # Handle case where last accumulation window is incomplete
    if micro_idx % accum_steps != 0:
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)
        step_count += 1

    result = {"model": model}
    if track_loss:
        result["train_loss_history"] = train_loss_history
    
    return result